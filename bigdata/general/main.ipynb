{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-KiQ7dVp1Fi",
        "outputId": "9bdda948-3a4c-4238-eff2-97d839c4bd23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.py\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "\n",
        "class DataLoader():\n",
        "    '''\n",
        "    Load BOJ dataset\n",
        "    '''\n",
        "    def __init__(self, path):        \n",
        "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
        "        print(self.pro_dir)\n",
        "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
        "\n",
        "        self.n_items = self.load_n_items()\n",
        "    \n",
        "    def load_data(self, datatype='train'):\n",
        "        if datatype == 'train':\n",
        "            return self._load_train_data()\n",
        "        elif datatype == 'validation':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        elif datatype == 'test':\n",
        "            return self._load_tr_te_data(datatype)\n",
        "        else:\n",
        "            raise ValueError(\"datatype should be in [train, validation, test]\")\n",
        "        \n",
        "    def load_n_items(self):\n",
        "        '''\n",
        "        아이템의 개수를 반환하는 함수.\n",
        "        '''\n",
        "        unique_sid = list()\n",
        "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                unique_sid.append(line.strip())\n",
        "        n_items = len(unique_sid)\n",
        "        return n_items\n",
        "    \n",
        "    def _load_train_data(self):\n",
        "        '''\n",
        "        train 데이터를 csr_matrix로 반환해주는 함수\n",
        "        '''\n",
        "        path = os.path.join(self.pro_dir, 'train.csv')\n",
        "        \n",
        "        tp = pd.read_csv(path)\n",
        "        n_users = tp['uid'].max() + 1\n",
        "\n",
        "        rows, cols = tp['uid'], tp['sid']\n",
        "        data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                                 (rows, cols)), dtype='float64',\n",
        "                                 shape=(n_users, self.n_items))\n",
        "        return data\n",
        "    \n",
        "    def _load_tr_te_data(self, datatype='test'):\n",
        "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
        "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
        "\n",
        "        tp_tr = pd.read_csv(tr_path)\n",
        "        tp_te = pd.read_csv(te_path)\n",
        "\n",
        "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "        \n",
        "        # print('start_idx :', start_idx)\n",
        "        # print('end_idx :', end_idx)\n",
        "\n",
        "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
        "        return data_tr, data_te"
      ],
      "metadata": {
        "id": "levOTynQ0TIm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bottleneck"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_XrhcM806Nq",
        "outputId": "c5bf578f-4750-4e70-ae8c-952ef9eb7436"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bottleneck\n",
            "  Downloading Bottleneck-1.3.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
            "\u001b[K     |████████████████████████████████| 355 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bottleneck) (1.21.6)\n",
            "Installing collected packages: bottleneck\n",
            "Successfully installed bottleneck-1.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.py\n",
        "\n",
        "import os\n",
        "from scipy import sparse\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import os\n",
        "import bottleneck as bn\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from math import log, floor, ceil\n",
        "\n",
        "\n",
        "def load_train_data(csv_file, n_items, n_users, global_indexing=False):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    \n",
        "    n_users = n_users if global_indexing else tp['uid'].max() + 1\n",
        "\n",
        "    rows, cols = tp['uid'], tp['sid']\n",
        "    data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                             (rows, cols)), dtype='float64',\n",
        "                             shape=(n_users, n_items))\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_tr_te_data(csv_file_tr, csv_file_te, n_items, n_users, global_indexing=False):\n",
        "    tp_tr = pd.read_csv(csv_file_tr)\n",
        "    tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "    if global_indexing:\n",
        "        start_idx = 0\n",
        "        end_idx = len(unique_uid) - 1\n",
        "    else:\n",
        "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    return data_tr, data_te\n",
        "\n",
        "\n",
        "def get_data(dataset, global_indexing=False):\n",
        "    unique_sid = list()\n",
        "    unique_uid = list()\n",
        "    with open(os.path.join(dataset, 'unique_sid.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            unique_sid.append(line.strip())\n",
        "\n",
        "    with open(os.path.join(dataset, 'unique_uid.txt'), 'r') as f:\n",
        "        for line in f:\n",
        "            unique_uid.append(line.strip())\n",
        "    \n",
        "    # unique_uid = list()\n",
        "    # with open(os.path.join(dataset, 'unique_uid.txt'), 'r') as f:\n",
        "    #     for line in f:\n",
        "    #         unique_uid.append(line.strip())\n",
        "            \n",
        "    n_items = len(unique_sid)\n",
        "    n_users = len(unique_uid)\n",
        "    \n",
        "    train_data = load_train_data(os.path.join(dataset, 'train.csv'), n_items, n_users, global_indexing=global_indexing)\n",
        "\n",
        "\n",
        "    vad_data_tr, vad_data_te = load_tr_te_data(os.path.join(dataset, 'validation_tr.csv'),\n",
        "                                               os.path.join(dataset, 'validation_te.csv'),\n",
        "                                               n_items, n_users, \n",
        "                                               global_indexing=global_indexing)\n",
        "\n",
        "    test_data_tr, test_data_te = load_tr_te_data(os.path.join(dataset, 'test_tr.csv'),\n",
        "                                                 os.path.join(dataset, 'test_te.csv'),\n",
        "                                                 n_items, n_users, \n",
        "                                                 global_indexing=global_indexing)\n",
        "    \n",
        "    data = train_data, vad_data_tr, vad_data_te, test_data_tr, test_data_te\n",
        "    data = (x.astype('float32') for x in data)\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_count(df, id):\n",
        "    '''\n",
        "    df -> DataFrame\n",
        "    id -> Feature of DataFrame\n",
        "    '''\n",
        "    interaction_count_groupby_id = df[[id]].groupby(id, as_index=False)\n",
        "    grouped_count = interaction_count_groupby_id.size()\n",
        "    return grouped_count\n",
        "\n",
        "# 특정한 횟수 이상의 리뷰가 존재하는(사용자의 경우 min_uc 이상, 아이템의 경우 min_sc이상) \n",
        "# 데이터만을 추출할 때 사용하는 함수입니다.\n",
        "# 현재 데이터셋에서는 결과적으로 원본그대로 사용하게 됩니다.\n",
        "def filter_triplets(df, min_user_interaction, min_problem_interaction):\n",
        "    user_interaction_count = get_count(df, 'user')\n",
        "    problem_interaction_count = get_count(df, 'item')\n",
        "\n",
        "    print(f\"Size of Dataframe Before Filtering: {df.size}\")\n",
        "\n",
        "    if min_user_interaction > 0:\n",
        "        df = df[df['user'].isin(user_interaction_count[user_interaction_count['size'] >= min_user_interaction]['user'])]\n",
        "\n",
        "    print(f\"Size of Dataframe After User Filtering: {df.size}\")\n",
        "\n",
        "    if min_problem_interaction > 0:\n",
        "        df = df[df['item'].isin(problem_interaction_count[problem_interaction_count['size'] >= min_problem_interaction]['item'])]\n",
        "\n",
        "    print(f\"Size of Dataframe After Problem Filtering: {df.size}\")\n",
        "\n",
        "    return df, user_interaction_count, problem_interaction_count\n",
        "\n",
        "#훈련된 모델을 이용해 검증할 데이터를 분리하는 함수입니다.\n",
        "#100개의 액션이 있다면, 그중에 test_prop 비율 만큼을 비워두고, 그것을 모델이 예측할 수 있는지를\n",
        "#확인하기 위함입니다.\n",
        "def split_train_test_proportion(data, test_prop=0.2):\n",
        "    '''\n",
        "    data -> DataFrame\n",
        "    \n",
        "    train과 test를 8:2 비율로 나눠주는 함수.\n",
        "    '''\n",
        "    data_grouped_by_user = data.groupby('user')\n",
        "    tr_list, te_list = list(), list()\n",
        "\n",
        "    np.random.seed(98765)\n",
        "    \n",
        "    for _, group in data_grouped_by_user:\n",
        "        n_items_u = len(group)\n",
        "        \n",
        "        if n_items_u >= 5:\n",
        "            idx = np.zeros(n_items_u, dtype='bool') # 'False'가 n_items_u개 만큼 채워진 array\n",
        "            \n",
        "            # n_items_u개 중에서 20%의 인덱스를 랜덤으로 뽑아서 해당 인덱스를 'True'로 바꿈\n",
        "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "                    \n",
        "            tr_list.append(group[np.logical_not(idx)]) # 'False'인 것을 tr_list에 추가\n",
        "            te_list.append(group[idx]) # 'True'인 것을 te_list에 추가\n",
        "        \n",
        "        else:\n",
        "            tr_list.append(group)\n",
        "    \n",
        "    data_tr = pd.concat(tr_list)\n",
        "    data_te = pd.concat(te_list)\n",
        "\n",
        "    return data_tr, data_te\n",
        "\n",
        "def numerize(tp, profile2id, show2id):\n",
        "    '''\n",
        "    tp -> DataFrame\n",
        "    user2id, item2id -> dict()\n",
        "    \n",
        "    user, item을 reindexing한 df 반환.\n",
        "    '''\n",
        "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
        "    sid = tp['item'].apply(lambda x: show2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
        "\n",
        "\n",
        "def sparse2torch_sparse(data):\n",
        "    \"\"\"\n",
        "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
        "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
        "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
        "    \"\"\"\n",
        "    samples = data.shape[0]\n",
        "    features = data.shape[1]\n",
        "    coo_data = data.tocoo()\n",
        "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
        "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
        "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
        "    values = np.array([row2val[r] for r in coo_data.row])\n",
        "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
        "    return t\n",
        "\n",
        "def naive_sparse2tensor(data):\n",
        "    return torch.FloatTensor(data.toarray())\n",
        "\n",
        "\n",
        "def numerize_for_infer(tp, profile2id, show2id):\n",
        "    uid = tp['user'].apply(lambda x: profile2id[str(x)])\n",
        "    sid = tp['item'].apply(lambda x: show2id[str(x)])\n",
        "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
        "\n",
        "def de_numerize(tp, re_p2id, re_s2id):\n",
        "    uid2 = tp['user'].apply(lambda x: re_p2id[x])\n",
        "    sid2 = tp['item'].apply(lambda x: re_s2id[x])\n",
        "    return pd.DataFrame(data={'uid': uid2, 'sid': sid2}, columns=['uid', 'sid'])\n",
        "\n",
        "def ndcg(X_pred, heldout_batch, k=100):\n",
        "    '''\n",
        "    Normalized Discounted Cumulative Gain@k for binary relevance\n",
        "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
        "    '''\n",
        "    batch_users = X_pred.shape[0]\n",
        "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
        "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "\n",
        "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
        "\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
        "\n",
        "    e = 0.000001\n",
        "\n",
        "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in heldout_batch.getnnz(axis=1)]) + e\n",
        "    return DCG / IDCG\n",
        "\n",
        "\n",
        "def recall(X_pred, heldout_batch, k=100):\n",
        "    batch_users = X_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
        "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
        "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
        "\n",
        "    e = 0.000001\n",
        "\n",
        "    X_true_binary = (heldout_batch > 0).toarray()\n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    #-------------------------\n",
        "    # print('tmp :', len(tmp))\n",
        "    # print('np.minimum(k, X_true_binary.sum(axis=1) :',len(np.minimum(k, X_true_binary.sum(axis=1))))\n",
        "    #-------------------------\n",
        "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1) + e)\n",
        "    return recall\n",
        "\n",
        "def min_max(lv):\n",
        "    if lv <= 5:\n",
        "        return 0, 7\n",
        "    elif lv <= 10:\n",
        "        return 5, 12\n",
        "    elif lv <= 13:\n",
        "        return 8, 16\n",
        "    elif lv <= 15:\n",
        "        return 11, 18\n",
        "    else:\n",
        "        return (floor(lv-log(lv, 2)), ceil(lv+log(lv, 3)))\n",
        "\n",
        "def infer(user, item, dict_user_lv, dict_item_lv, id2show, infer_cnt):\n",
        "    pred = np.array([])\n",
        "    user_lv = dict_user_lv[user]\n",
        "    cnt = 0\n",
        "    mini, maxi = min_max(user_lv)\n",
        "    item = sorted(item, reverse=True)\n",
        "    for i in item:\n",
        "        item_lv = dict_item_lv[int(id2show[i])]\n",
        "        if mini <= item_lv <= maxi:\n",
        "            pred = np.append(pred, i)\n",
        "            cnt += 1\n",
        "    \n",
        "        if cnt == infer_cnt:\n",
        "            return pred\n",
        "    else:\n",
        "        # print('else user :', user)\n",
        "        if len(pred) < infer_cnt:\n",
        "            for i in item:\n",
        "                if i not in pred:\n",
        "                    pred = np.append(pred, i)\n",
        "\n",
        "                if len(pred) == infer_cnt:\n",
        "                    return np.array(pred)"
      ],
      "metadata": {
        "id": "IikL3HWB0ouh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config.py\n",
        "\n",
        "# ====================================================\n",
        "# CFG\n",
        "# ====================================================\n",
        "class args:\n",
        "    dataset = '/home/recognizer14/airflow/dags/module_models/rec_general'\n",
        "    hidden_dim = 600\n",
        "    latent_dim = 200\n",
        "    batch_size = 500\n",
        "    beta = None\n",
        "    gamma = 0.005\n",
        "    lr = 5e-4\n",
        "    n_epochs = 10\n",
        "    n_enc_epochs = 3\n",
        "    n_dec_epochs = 1\n",
        "    not_alternating = False\n",
        "    data = 'data/train/'\n",
        "    cuda = False\n",
        "    seed = 1111\n",
        "    early_stopping = 5\n",
        "    infer_cnt = 20\n",
        "    save = 'model.pt'\n",
        "    total_anneal_steps = 200000\n",
        "    anneal_cap = 0.2\n",
        "    log_dir = './logs'\n",
        "    wd = 0.00\n",
        "    log_interval = 100\n",
        "    save_dir = '/home/recognizer14/airflow/dags/module_models/rec_general/best_models'"
      ],
      "metadata": {
        "id": "v0SiZW6G1i_I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing.py\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "#from .dataset import *\n",
        "#from .utils import *\n",
        "import json\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "## 각종 파라미터 세팅\n",
        "#from .config import *\n",
        "\n",
        "# Set the random seed manually for reproductibility.\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "def preprocessing_all(raw_data, df_problems, db):\n",
        "    #만약 GPU가 사용가능한 환경이라면 GPU를 사용\n",
        "    if torch.cuda.is_available():\n",
        "        args.cuda = True\n",
        "\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "    print(\"Load and Preprocess BOJ dataset\")\n",
        "    # Load Data\n",
        "    # Filter Data\n",
        "#    raw_data, user_activity, item_popularity = filter_triplets(raw_data, 5, 10)\n",
        "    #---------------------------\n",
        "    # 여기에 tag == None인 아이템은 제외\n",
        "#    df_problems = pd.read_sql('select * from problems', db)\n",
        "#    df_problems.drop(df_problems[df_problems.average_tries == 7340].index, axis=0, inplace=True)\n",
        "    # level 0에 해당하는 문제 제거\n",
        "#    df_problems = df_problems[df_problems.level != 0]\n",
        "    # not_solvable == False만\n",
        "#    df_problems = df_problems[df_problems.is_solvable == True]\n",
        "    # tag가 nan인 문제 제거\n",
        "#    df_problems = df_problems[~df_problems.tags.isnull()]\n",
        "\n",
        " #   raw_data = raw_data[raw_data['item'].isin(df_problems['problem_id'].values)].reset_index(drop=True)\n",
        "    #---------------------------\n",
        "\n",
        "\n",
        "    # Shuffle User Indices\n",
        "    unique_uid = pd.unique(raw_data['user'])\n",
        "    print(\"len(unique_uid): \", len(unique_uid))\n",
        "    print(\"(BEFORE) unique_uid:\",unique_uid)\n",
        "    np.random.seed(98765)\n",
        "    idx_perm = np.random.permutation(unique_uid.size) # 해당 숫자까지의 인덱스를 무작위로 섞은 것을 arr로 반환\n",
        "    unique_uid = unique_uid[idx_perm]\n",
        "    print(\"(AFTER) unique_uid:\",unique_uid) # 무작위로 item을 섞음\n",
        "\n",
        "    n_users = unique_uid.size\n",
        "    n_heldout_users = 3000\n",
        "\n",
        "    # Split Train/Validation/Test User Indices\n",
        "    tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "    vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
        "    te_users = unique_uid[(n_users - n_heldout_users):]\n",
        "\n",
        "    #주의: 데이터의 수가 아닌 사용자의 수입니다!\n",
        "    print(\"훈련 데이터에 사용될 사용자 수:\", len(tr_users))\n",
        "    print(\"검증 데이터에 사용될 사용자 수:\", len(vd_users))\n",
        "    print(\"테스트 데이터에 사용될 사용자 수:\", len(te_users))\n",
        "\n",
        "    ##훈련 데이터에 해당하는 아이템들\n",
        "    #Train에는 전체 데이터를 사용합니다.\n",
        "    train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
        "\n",
        "    ##아이템 ID\n",
        "    unique_sid = pd.unique(raw_data['item'])\n",
        "    pro_dir = os.path.join(args.dataset, 'pro_sg')\n",
        "\n",
        "    if not os.path.exists(pro_dir):\n",
        "        os.makedirs(pro_dir)\n",
        "\n",
        "    item2id = dict((int(sid), int(i)) for (i, sid) in enumerate(unique_sid)) # item2idx dict\n",
        "    user2id = dict((pid, int(i)) for (i, pid) in enumerate(unique_uid)) # user2idx dict\n",
        "\n",
        "    with open(os.path.join(pro_dir, 'item2id.json'), 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(item2id, f, ensure_ascii=False, indent=\"\\t\")\n",
        "        \n",
        "    with open(os.path.join(pro_dir, 'user2id.json'), 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(user2id, f, ensure_ascii=False, indent=\"\\t\")\n",
        "\n",
        "    with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "        for sid in unique_sid:\n",
        "            f.write('%s\\n' % sid)\n",
        "\n",
        "    with open(os.path.join(pro_dir, 'unique_uid.txt'), 'w') as f:\n",
        "        for uid in unique_uid:\n",
        "            f.write('%s\\n' % uid)\n",
        "\n",
        "    #Validation과 Test에는 input으로 사용될 tr 데이터와 정답을 확인하기 위한 te 데이터로 분리되었습니다.\n",
        "    print('Data Split Start!')\n",
        "    vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
        "    vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
        "    vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
        "\n",
        "    test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
        "    test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
        "    test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
        "\n",
        "    train_data = numerize(train_plays, user2id, item2id)\n",
        "    train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
        "\n",
        "    vad_data_tr = numerize(vad_plays_tr, user2id, item2id)\n",
        "    vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
        "\n",
        "    vad_data_te = numerize(vad_plays_te, user2id, item2id)\n",
        "    vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
        "\n",
        "    test_data_tr = numerize(test_plays_tr, user2id, item2id)\n",
        "    test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
        "\n",
        "    test_data_te = numerize(test_plays_te, user2id, item2id)\n",
        "    test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
        "    print(\"Data Split Done!\")\n",
        "\n",
        "    # item_tag_emb\n",
        "    print(\"Item tag emb start!\")\n",
        "    set_tags = set()\n",
        "    for tags in df_problems['tags'].dropna().values:\n",
        "        for tag in tags.split(','):\n",
        "            set_tags.add(tag)\n",
        "\n",
        "\n",
        "    df_tags = df_problems[['problem_id', 'tags']]\n",
        "    df_tags['tags'] = df_tags['tags'].str.split(',')\n",
        "    # df_tags = df_tags.explode('tags').dropna().reset_index(drop=True)\n",
        "    df_tags = df_tags.explode('tags').dropna().reset_index(drop=True)\n",
        "    df_tags = df_tags[df_tags['problem_id'].isin(unique_sid)].reset_index(drop=True)\n",
        "\n",
        "    temp = nn.Embedding(len(set_tags), 300)\n",
        "    tag_emb = pd.DataFrame(df_tags['tags'].value_counts().index.values, columns=['tags'])\n",
        "\n",
        "    dict_tag_idx = dict()\n",
        "    for i, j in enumerate(df_tags['tags'].value_counts().index.values):\n",
        "        dict_tag_idx[j] = i\n",
        "\n",
        "    list_emb = []\n",
        "    dict_tag_emb = dict()\n",
        "    for i in df_tags['tags'].value_counts().index.values:\n",
        "        list_emb.append(temp(torch.tensor(dict_tag_idx[i])).detach().numpy())\n",
        "        dict_tag_emb[i] = temp(torch.tensor(dict_tag_idx[i])).detach().numpy()\n",
        "\n",
        "    df_tag_emb = pd.concat([tag_emb, pd.DataFrame(list_emb)], axis=1)\n",
        "    df_tags2 = pd.merge(df_tags, df_tag_emb, on='tags', how='left')\n",
        "    tag2emb = df_tags2.iloc[:, 2:].values\n",
        "    df_tags['emb'] = list(tag2emb) \n",
        "\n",
        "    total = []\n",
        "    def item_genre_emb_mean(i):\n",
        "        total.append(np.mean(df_tags[df_tags['problem_id'] == i].emb))\n",
        "\n",
        "    item_genre_emb_idx = pd.DataFrame(list(df_tags['problem_id'].unique()), columns=['item'])\n",
        "    item_genre_emb_idx.item.apply(lambda x: item_genre_emb_mean(x))\n",
        "    item_genre_emb = pd.DataFrame(total)\n",
        "    item_genre_emb.index = df_tags['problem_id'].unique()\n",
        "\n",
        "    item_genre_emb = item_genre_emb.reset_index()\n",
        "    item_genre_emb['index'] = item_genre_emb['index'].apply(lambda x : item2id[x])\n",
        "    item_genre_emb = item_genre_emb.set_index('index')\n",
        "    item_genre_emb = item_genre_emb.sort_index()\n",
        "\n",
        "    item_genre_emb = item_genre_emb.T\n",
        "    print(item_genre_emb.shape)\n",
        "\n",
        "    item_genre_emb.to_csv(pro_dir + '/item_tag_emb.csv', index=False)\n",
        "    print('Item tag emb Done!')\n",
        "\n",
        "    model_score = dict()\n",
        "    model_score['recvae'] = 0\n",
        "    model_score['vae'] = 0\n",
        "    model_score['dae'] = 0\n",
        "    with open(os.path.join(pro_dir, 'model_score.json'), 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(model_score, f, ensure_ascii=False, indent=\"\\t\")\n",
        "    #--------------------------------------------"
      ],
      "metadata": {
        "id": "M8f5OuHT0BGK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# models.py\n",
        "\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# RecVAE\n",
        "# ------------------------------------------------------------------\n",
        "def swish(x):\n",
        "    return x.mul(torch.sigmoid(x))\n",
        "\n",
        "def log_norm_pdf(x, mu, logvar):\n",
        "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
        "\n",
        "\n",
        "class CompositePrior(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
        "        super(CompositePrior, self).__init__()\n",
        "\n",
        "        self.mixture_weights = mixture_weights\n",
        "\n",
        "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.mu_prior.data.fill_(0)\n",
        "\n",
        "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.logvar_prior.data.fill_(0)\n",
        "\n",
        "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
        "        self.logvar_uniform_prior.data.fill_(10)\n",
        "\n",
        "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.encoder_old.requires_grad_(False)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        post_mu, post_logvar = self.encoder_old(x, 0)\n",
        "\n",
        "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
        "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
        "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
        "\n",
        "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
        "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
        "\n",
        "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
        "\n",
        "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x, dropout_rate):\n",
        "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
        "        x = x / norm[:, None]\n",
        "\n",
        "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
        "\n",
        "        h1 = self.ln1(swish(self.fc1(x)))\n",
        "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
        "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
        "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
        "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
        "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
        "\n",
        "\n",
        "class RecVAE(nn.Module):\n",
        "    def __init__(self, hidden_dim, latent_dim, input_dim):\n",
        "        super(RecVAE, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
        "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
        "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5*logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def forward(self, user_ratings, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True):\n",
        "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        x_pred = self.decoder(z)\n",
        "\n",
        "        if calculate_loss:\n",
        "            if gamma:\n",
        "                norm = user_ratings.sum(dim=-1)\n",
        "                kl_weight = gamma * norm\n",
        "            elif beta:\n",
        "                kl_weight = beta\n",
        "\n",
        "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
        "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
        "            negative_elbo = -(mll - kld)\n",
        "\n",
        "            return (mll, kld), negative_elbo\n",
        "\n",
        "        else:\n",
        "            return x_pred\n",
        "\n",
        "    def update_prior(self):\n",
        "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))\n",
        "\n",
        "# Multi-VAE\n",
        "# ------------------------------------------------------------------\n",
        "class MultiVAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-VAE\n",
        "    Multi-VAE : Variational Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, tag_emb=None, title_emb=None, dropout=0.5):\n",
        "        super(MultiVAE, self).__init__()\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.item_tag = torch.Tensor(tag_emb.values).to(self.device)\n",
        "\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1]\n",
        "\n",
        "        # Last dimension of q- network is for mean and variance\n",
        "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
        "\n",
        "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "                                       d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
        "\n",
        "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "                                       d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "\n",
        "        mu, logvar = self.encode(self.input)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "    def encode(self, input):\n",
        "        h = F.normalize(input)\n",
        "        h = self.drop(h)\n",
        "        h = h.to(self.device)\n",
        "        h = torch.cat((self.item_tag, h), 0)\n",
        "\n",
        "        mu, logvar = None, None\n",
        "        for i, layer in enumerate(self.q_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.q_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "            else:\n",
        "                mu = h[:, :self.q_dims[-1]]\n",
        "                logvar = h[:, self.q_dims[-1]:]\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:  # 이게 뭐지..?\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = z\n",
        "        for i, layer in enumerate(self.p_layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.p_layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "\n",
        "        item_tag_emb, reconstructed_h = h.split(\n",
        "            [self.item_tag.shape[0], self.input.shape[0]], 0)\n",
        "        return reconstructed_h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.q_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0 / (fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "        for layer in self.p_layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0 / (fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "\n",
        "\n",
        "def loss_function_vae(recon_x, x, mu, logvar, anneal=1.0):\n",
        "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
        "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
        "\n",
        "    return BCE + anneal * KLD\n",
        "\n",
        "# Multi-DAE\n",
        "# ------------------------------------------------------------------\n",
        "class MultiDAE(nn.Module):\n",
        "    \"\"\"\n",
        "    Container module for Multi-DAE\n",
        "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
        "    See Variational Autoencoders for Collaborative Filtering\n",
        "    https://arxiv.org/abs/1802.05814\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p_dims, q_dims=None, tag_emb=None, dropout=0.5):\n",
        "        super(MultiDAE, self).__init__()\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.item_tag = torch.Tensor(tag_emb.values).to(self.device)\n",
        "        self.p_dims = p_dims\n",
        "        if q_dims:\n",
        "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
        "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
        "            self.q_dims = q_dims\n",
        "        else:\n",
        "            self.q_dims = p_dims[::-1]  # 리스트를 역으로 뒤집음\n",
        "\n",
        "        self.dims = self.q_dims + self.p_dims[1:]\n",
        "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
        "                                     d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input):\n",
        "        h = F.normalize(input)\n",
        "        h = self.drop(h)\n",
        "        h = h.to(self.device)\n",
        "        h = torch.cat((self.item_tag, h), 0)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            h = layer(h)\n",
        "            if i != len(self.layers) - 1:\n",
        "                h = F.tanh(h)\n",
        "\n",
        "        item_tag_emb, reconstructed_h = h.split([self.item_tag.shape[0], input.shape[0]], 0)\n",
        "        return reconstructed_h\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.layers:\n",
        "            # Xavier Initialization for weights\n",
        "            size = layer.weight.size()\n",
        "            fan_out = size[0]\n",
        "            fan_in = size[1]\n",
        "            std = np.sqrt(2.0 / (fan_in + fan_out))\n",
        "            layer.weight.data.normal_(0.0, std)\n",
        "\n",
        "            # Normal Initialization for Biases\n",
        "            layer.bias.data.normal_(0.0, 0.001)\n",
        "\n",
        "\n",
        "\n",
        "def loss_function_dae(recon_x, x):\n",
        "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
        "    return BCE"
      ],
      "metadata": {
        "id": "Yq6zFzcV12Dk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inference.py\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "import os\n",
        "#from .dataset import *\n",
        "#from .utils import *\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "#from .models import *\n",
        "import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "import bottleneck as bn\n",
        "\n",
        "## 각종 파라미터 세팅\n",
        "#from .config import *\n",
        "\n",
        "seed = args.seed\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def infer_all(raw_data, df_problems, db):\n",
        "    ## 배치사이즈 포함\n",
        "    ### 데이터 준비\n",
        "    # Load Data\n",
        "    pro_dir = os.path.join(args.dataset, 'pro_sg')\n",
        "\n",
        "    with open(pro_dir + '/model_score.json', 'r', encoding=\"utf-8\") as f:\n",
        "        model_score = json.load(f)\n",
        "\n",
        "#    raw_data, _, _ = filter_triplets(raw_data, 5, 10)\n",
        "#    df_problems = pd.read_sql('select * from problems', db)\n",
        "#    df_problems.drop(df_problems[df_problems.average_tries == 7340].index, axis=0, inplace=True)\n",
        "#    # level 0에 해당하는 문제 제거\n",
        "#    df_problems = df_problems[df_problems.level != 0]\n",
        "#    # not_solvable == False만\n",
        "#    df_problems = df_problems[df_problems.is_solvable == True]\n",
        "#    # tag가 nan인 문제 제거\n",
        "#    df_problems = df_problems[~df_problems.tags.isnull()]\n",
        "#\n",
        "#    raw_data = raw_data[raw_data['item'].isin(df_problems['problem_id'].values)].reset_index(drop=True)\n",
        "    print(\"raw_data nunique:\", raw_data.user.nunique())\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        args.cuda = True\n",
        "#    device = torch.device(\"cuda\")\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    #device = \"cpu\"\n",
        "\n",
        "    # Import Data\n",
        "    print(\"Inference Start!!\")\n",
        "\n",
        "    with open(pro_dir + '/item2id.json', 'r', encoding=\"utf-8\") as f:\n",
        "        show2id = json.load(f)\n",
        "\n",
        "    with open(pro_dir + '/user2id.json', 'r', encoding=\"utf-8\") as f:\n",
        "        profile2id = json.load(f)\n",
        "\n",
        "    infer_df = numerize_for_infer(raw_data, profile2id, show2id)\n",
        "\n",
        "    loader = DataLoader(args.dataset)\n",
        "    n_items = loader.load_n_items()\n",
        "    n_users = infer_df['uid'].max() + 1\n",
        "\n",
        "    rows, cols = infer_df['uid'], infer_df['sid']\n",
        "    data = sparse.csr_matrix((np.ones_like(rows),(rows, cols)), dtype='float64', shape=(n_users, n_items))\n",
        "\n",
        "    num_data = data.shape[0]\n",
        "    index_list = list(range(num_data))\n",
        "\n",
        "    df_users = pd.read_sql('select * from users', db)\n",
        "    df_users = df_users[['handle', 'tier']] # base_dir 경로를 수정해야 합니다.\n",
        "    df_user_lv = df_users[df_users['handle'].isin(raw_data['user'].unique())].reset_index(drop=True)\n",
        "\n",
        "    dict_user_lv = dict()\n",
        "    for i in range(len(df_user_lv)):\n",
        "        dict_user_lv[df_user_lv.iloc[i,0]] = df_user_lv.iloc[i, 1]\n",
        "\n",
        "    dict_item_lv = dict()\n",
        "    for i in range(len(df_problems)):\n",
        "        dict_item_lv[df_problems.iloc[i, 1]] = df_problems.iloc[i,6]\n",
        "\n",
        "    id2profile = dict((int(v), k) for k, v in profile2id.items())\n",
        "    id2show = dict((int(v), k) for k, v in show2id.items())\n",
        "\n",
        "    model_kwargs = {\n",
        "    'hidden_dim': args.hidden_dim,\n",
        "    'latent_dim': args.latent_dim,\n",
        "    'input_dim': data.shape[1]\n",
        "    }\n",
        "\n",
        "    model_name = \"\"\n",
        "    score = 0\n",
        "    for m, s in model_score.items():\n",
        "        if s > score:\n",
        "            print(\"확인\")\n",
        "            model_name = m\n",
        "            score = s\n",
        "        print(m, s)\n",
        "        print(score)\n",
        "    print(f\"Best Model => {model_name} : {score:.4f}\")\n",
        "\n",
        "    if model_name == 'recvae':        \n",
        "        model = RecVAE(**model_kwargs)\n",
        "        model.load_state_dict(torch.load(args.save_dir + '/best_recvae_' + args.save))\n",
        "        model = model.to(device)\n",
        "    \n",
        "    elif model_name == 'vae':\n",
        "        p_dims = [200, 3000, n_items] # [200, 600, 6807]\n",
        "        item_tag_emb = pd.read_csv(pro_dir + '/item_tag_emb.csv')\n",
        "\n",
        "        model = MultiVAE(p_dims, tag_emb=item_tag_emb)\n",
        "\n",
        "        with open(os.path.join(args.save_dir, 'best_vae_' + args.save), 'rb') as f:\n",
        "            model.load_state_dict(torch.load(f))\n",
        "        model = model.to(device)\n",
        "\n",
        "    elif model_name == 'dae':\n",
        "        p_dims = [200, 3000, n_items]  # [200, 600, 6807]\n",
        "        item_tag_emb = pd.read_csv(pro_dir + '/item_tag_emb.csv')\n",
        "\n",
        "        model = MultiDAE(p_dims, tag_emb=item_tag_emb).to(device)\n",
        "\n",
        "        with open(os.path.join(args.save_dir, 'best_dae_' + args.save), 'rb') as f:\n",
        "            model.load_state_dict(torch.load(f))\n",
        "        model = model.to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Infer\n",
        "    pred_list = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for start_index in tqdm(range(0, num_data, args.batch_size)):\n",
        "            end_index = min(start_index + args.batch_size, num_data)\n",
        "            data_batch = data[index_list[start_index:end_index]]\n",
        "            data_tensor = naive_sparse2tensor(data_batch).to(device)\n",
        "\n",
        "            #recon_batch = model(data_tensor, calculate_loss=False)\n",
        "            if model_name == 'vae':\n",
        "                 recon_batch, _, _ = model(data_tensor)\n",
        "            elif model_name == 'dae':\n",
        "                 recon_batch = model(data_tensor)\n",
        "            else:\n",
        "                 recon_batch = model(data_tensor, calculate_loss=False)\n",
        "\n",
        "            recon_batch = recon_batch.cpu().numpy()\n",
        "\n",
        "            recon_batch[data_batch.nonzero()] = -np.inf\n",
        "\n",
        "            idx = bn.argpartition(-recon_batch, 500, axis=1)[:, :500]\n",
        "            out = np.array([])\n",
        "\n",
        "            for user, item in enumerate(idx):\n",
        "                user += start_index\n",
        "                user = id2profile[user]\n",
        "\n",
        "                infer_out = infer(user, item, dict_user_lv, dict_item_lv, id2show, args.infer_cnt)\n",
        "                out = np.append(out, infer_out)\n",
        "\n",
        "\n",
        "            if start_index == 0:\n",
        "                pred_list = out\n",
        "            else:\n",
        "                pred_list = np.append(pred_list, np.array(out))\n",
        "\n",
        "    user2 = []\n",
        "    item2 = []\n",
        "\n",
        "    pred_list = pred_list.reshape(num_data, -1)\n",
        "    for user_index, pred_item in enumerate(pred_list):\n",
        "        user2.extend([user_index] * args.infer_cnt)\n",
        "        item2.extend(pred_item)\n",
        "\n",
        "    u2 = pd.DataFrame(user2, columns=['user'])\n",
        "    i2 = pd.DataFrame(item2, columns=['item'])\n",
        "    all2 = pd.concat([u2, i2], axis=1)\n",
        "\n",
        "    ans2 = de_numerize(all2, id2profile, id2show)\n",
        "    ans2.columns = ['user', 'item']\n",
        "    new_ans2 = ans2.sort_values('user')\n",
        "\n",
        "    new_ans2.to_csv(os.path.join(args.dataset, 'output.csv'), index=False)\n",
        "    # print('Output Saved!!')\n",
        "    return new_ans2"
      ],
      "metadata": {
        "id": "8uticbIh1sm2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_recvae.py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "import random\n",
        "from copy import deepcopy\n",
        "#from .utils import *\n",
        "#from .models import *\n",
        "import time\n",
        "#from .config import *\n",
        "#from .dataset import *\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "seed = args.seed\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "# 만약 GPU가 사용가능한 환경이라면 GPU를 사용\n",
        "def train_recvae():\n",
        "    if torch.cuda.is_available():\n",
        "        args.cuda = True\n",
        "\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    #device = \"cpu\"\n",
        "    print(\"DEVICE: \", device)\n",
        "    \n",
        "    def evaluate(model, data_tr, data_te, metrics, batch_size=500):\n",
        "        metrics = deepcopy(metrics)\n",
        "        model.eval()\n",
        "\n",
        "        e_idxlist = list(range(data_tr.shape[0]))\n",
        "        e_N = data_tr.shape[0]\n",
        "\n",
        "        for m in metrics:\n",
        "            m['score'] = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for start_idx in range(0, e_N, batch_size):\n",
        "                end_idx = min(start_idx + batch_size, e_N)\n",
        "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
        "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
        "\n",
        "                data_tensor = naive_sparse2tensor(data).to(device)\n",
        "\n",
        "                ratings_pred = model(data_tensor, calculate_loss=False).cpu().detach().numpy()\n",
        "\n",
        "                if not (data_tr is data_te):\n",
        "                    ratings_pred[data.nonzero()] = -np.inf\n",
        "\n",
        "                for m in metrics:\n",
        "                    m['score'].append(m['metric'](ratings_pred, heldout_data, k=m['k']))\n",
        "\n",
        "        for m in metrics:\n",
        "            m['score'] = np.concatenate(m['score']).mean()\n",
        "\n",
        "        return [x['score'] for x in metrics]\n",
        "\n",
        "\n",
        "    def train(model, opts, train_data, batch_size, beta, gamma, dropout_rate):\n",
        "        model.train()\n",
        "        np.random.shuffle(idxlist)\n",
        "        for batch_idx, start_idx in enumerate(range(0, N, batch_size)):\n",
        "            end_idx = min(start_idx + batch_size, N)\n",
        "\n",
        "            data = train_data[idxlist[start_idx:end_idx]]\n",
        "            data = naive_sparse2tensor(data).to(device)\n",
        "\n",
        "            for optimizer in opts:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            _, loss = model(data, beta=beta, gamma=gamma, dropout_rate=dropout_rate)\n",
        "            \n",
        "            loss.backward()\n",
        "\n",
        "            for optimizer in opts:\n",
        "                optimizer.step()\n",
        "\n",
        "    loader = DataLoader(args.dataset)\n",
        "    print(\"loader 완료\")\n",
        "    n_items = loader.load_n_items()\n",
        "    train_data = loader.load_data('train')\n",
        "    vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "    test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "    N = train_data.shape[0]  # 25360\n",
        "    idxlist = list(range(N))\n",
        "\n",
        "    model_kwargs = {\n",
        "        'hidden_dim': args.hidden_dim,\n",
        "        'latent_dim': args.latent_dim,\n",
        "        'input_dim': train_data.shape[1]\n",
        "    }\n",
        "\n",
        "    metrics = [\n",
        "        {'metric': ndcg, 'k': 100},\n",
        "        {'metric': recall, 'k': 10},\n",
        "        {'metric': recall, 'k': 30},\n",
        "        {'metric': recall, 'k': 50}\n",
        "    ]\n",
        "\n",
        "    best_recall = -np.inf\n",
        "\n",
        "    model = RecVAE(**model_kwargs).to(device)\n",
        "    model_best = RecVAE(**model_kwargs).to(device)\n",
        "\n",
        "    learning_kwargs = {\n",
        "        'model': model,\n",
        "        'train_data': train_data,\n",
        "        'batch_size': args.batch_size,\n",
        "        'beta': args.beta,\n",
        "        'gamma': args.gamma\n",
        "    }\n",
        "\n",
        "    decoder_params = set(model.decoder.parameters())\n",
        "    encoder_params = set(model.encoder.parameters())\n",
        "\n",
        "    optimizer_encoder = optim.Adam(encoder_params, lr=args.lr)\n",
        "    optimizer_decoder = optim.Adam(decoder_params, lr=args.lr)\n",
        "\n",
        "    log_dir_name = str(datetime.datetime.now())[0:10] + f'_recvae'\n",
        "    log_dir = os.path.join(args.log_dir, log_dir_name)\n",
        "\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "    # -- wandb initialize with configuration\n",
        "\n",
        "    for epoch in range(1, args.n_epochs+1):\n",
        "        if args.not_alternating:\n",
        "            train(opts=[optimizer_encoder, optimizer_decoder], dropout_rate=0.5, **learning_kwargs)\n",
        "        else:\n",
        "            train(opts=[optimizer_encoder], dropout_rate=0.5, **learning_kwargs)\n",
        "            model.update_prior()\n",
        "            train(opts=[optimizer_decoder], dropout_rate=0, **learning_kwargs)\n",
        "\n",
        "        train_scores = evaluate(model, train_data, train_data, metrics)\n",
        "        valid_scores = evaluate(model, vad_data_tr, vad_data_te, metrics)\n",
        "\n",
        "        print('-' * 70)\n",
        "        print(f'epoch {epoch} | previous best recall: {best_recall}')\n",
        "\n",
        "        print('training set')\n",
        "        for metric, score in zip(metrics, train_scores):\n",
        "            print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")\n",
        "\n",
        "        valid_recall = 0\n",
        "\n",
        "        print('validation set')\n",
        "        for metric, score in zip(metrics, valid_scores):\n",
        "            if metric['metric'].__name__ == 'recall' and metric['k'] == 30:\n",
        "                valid_recall = score\n",
        "            print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")\n",
        "\n",
        "        if valid_recall > best_recall:\n",
        "            best_recall = valid_recall\n",
        "            model_best.load_state_dict(deepcopy(model.state_dict()))\n",
        "            with open(os.path.join(log_dir, f'best_recvae_' + args.save), 'wb') as f:\n",
        "                torch.save(model.state_dict(), f)\n",
        "            print(f\"best_model saved!! (save {best_recall})\")\n",
        "        print('-' * 70 + '\\n')\n",
        "\n",
        "    with open(os.path.join(args.save_dir, f'best_recvae_' + args.save), 'wb') as f:\n",
        "        torch.save(model_best.state_dict(), f)\n",
        "\n",
        "    final_scores = evaluate(model_best, test_data_tr, test_data_te, metrics)\n",
        "\n",
        "    for metric, score in zip(metrics, final_scores):\n",
        "        print(f\"{metric['metric'].__name__}@{metric['k']}:\\t{score:.4f}\")\n",
        "\n",
        "    pro_dir = os.path.join(args.dataset, 'pro_sg')\n",
        "    #np.savetxt(pro_dir+'/recvae_result.txt', [final_scores[1]]) # best_recall 저장\n",
        "\n",
        "    #f = open(f\"{pro_dir}/model_score.json\", encoding=\"utf-8\")\n",
        "    #model_score = json.loads(f.read())\n",
        "    with open(pro_dir + '/model_score.json', 'r', encoding=\"utf-8\") as f:\n",
        "        model_score = json.load(f)\n",
        "\n",
        "    model_score['recvae'] = final_scores[1]\n",
        "    with open(pro_dir + '/model_score.json', 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(model_score, f, ensure_ascii=False, indent=\"\\t\")"
      ],
      "metadata": {
        "id": "1z166-fQ2C6z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_vae.py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "import random\n",
        "from copy import deepcopy\n",
        "#from .utils import *\n",
        "#from .models import *\n",
        "import time\n",
        "#from .config import *\n",
        "#from .dataset import *\n",
        "import datetime\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "seed = args.seed\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "update_count = 0\n",
        "def train_vae():\n",
        "    # 만약 GPU가 사용가능한 환경이라면 GPU를 사용\n",
        "    if torch.cuda.is_available():\n",
        "        args.cuda = True\n",
        "    #args.cuda = False\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    #device = \"cpu\"\n",
        "    print(\"DEVICE: \", device)\n",
        "    \n",
        "    def train(model, criterion, optimizer, is_VAE = False):\n",
        "        # Turn on training mode\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        start_time = time.time()\n",
        "        global update_count\n",
        "\n",
        "        np.random.shuffle(idxlist)\n",
        "        \n",
        "        for batch_idx, start_idx in enumerate(range(0, N, args.batch_size)):\n",
        "            end_idx = min(start_idx + args.batch_size, N)\n",
        "            \n",
        "            data = train_data[idxlist[start_idx:end_idx]]\n",
        "            data = naive_sparse2tensor(data).to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if is_VAE:\n",
        "                if args.total_anneal_steps > 0:\n",
        "                    anneal = min(args.anneal_cap, \n",
        "                                    1. * update_count / args.total_anneal_steps)\n",
        "                else:\n",
        "                    anneal = args.anneal_cap\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                recon_batch, mu, logvar = model(data)\n",
        "                loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
        "            else:\n",
        "                recon_batch = model(data)\n",
        "                loss = criterion(recon_batch, data)\n",
        "\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "            update_count += 1\n",
        "\n",
        "\n",
        "    def evaluate(model, criterion, data_tr, data_te, is_VAE=False):\n",
        "        # Turn on evaluation mode\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        global update_count\n",
        "        e_idxlist = list(range(data_tr.shape[0]))\n",
        "        e_N = data_tr.shape[0]\n",
        "        n100_list = []\n",
        "        r10_list = []\n",
        "        r30_list = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for start_idx in range(0, e_N, args.batch_size):\n",
        "                end_idx = min(start_idx + args.batch_size, N)\n",
        "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
        "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
        "\n",
        "                data_tensor = naive_sparse2tensor(data).to(device)\n",
        "                if is_VAE :\n",
        "\n",
        "                    if args.total_anneal_steps > 0:\n",
        "                        anneal = min(args.anneal_cap, \n",
        "                                    1. * update_count / args.total_anneal_steps)\n",
        "                    else:\n",
        "                        anneal = args.anneal_cap\n",
        "\n",
        "                    recon_batch, mu, logvar = model(data_tensor)\n",
        "\n",
        "                    loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
        "\n",
        "                else :\n",
        "                    recon_batch = model(data_tensor)\n",
        "                    loss = criterion(recon_batch, data_tensor)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Exclude examples from training set\n",
        "                recon_batch = recon_batch.cpu().numpy()\n",
        "                recon_batch[data.nonzero()] = -np.inf\n",
        "\n",
        "                n100 = ndcg(recon_batch, heldout_data, 100)\n",
        "                r10 = recall(recon_batch, heldout_data, 10)\n",
        "                r30 = recall(recon_batch, heldout_data, 30)\n",
        "\n",
        "                n100_list.append(n100)\n",
        "                r10_list.append(r10)\n",
        "                r30_list.append(r30)\n",
        "    \n",
        "        total_loss /= len(range(0, e_N, args.batch_size))\n",
        "        n100_list = np.concatenate(n100_list)\n",
        "        r10_list = np.concatenate(r10_list)\n",
        "        r30_list = np.concatenate(r30_list)\n",
        "\n",
        "        return total_loss, np.mean(n100_list), np.mean(r10_list), np.mean(r30_list)\n",
        "\n",
        "    ###############################################################################\n",
        "    # Load data\n",
        "    ###############################################################################\n",
        "    loader = DataLoader(args.dataset)\n",
        "\n",
        "    n_items = loader.load_n_items() # 6807\n",
        "    train_data = loader.load_data('train') # csr_matrix\n",
        "    vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "    test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "    N = train_data.shape[0] # 25360\n",
        "    idxlist = list(range(N))\n",
        "\n",
        "    ###############################################################################\n",
        "    # Build the model\n",
        "    ###############################################################################\n",
        "\n",
        "    # p_dims = [200, 1200, 3000, n_items] # [200, 600, 6807]\n",
        "    p_dims = [200, 3000, n_items] # [200, 600, 6807]\n",
        "    pro_dir2 = os.path.join(args.dataset, 'pro_sg')\n",
        "    item_tag_emb = pd.read_csv(pro_dir2 + '/item_tag_emb.csv')\n",
        "    # model = MultiVAE(p_dims).to(device)\n",
        "    model = MultiVAE(p_dims, tag_emb=item_tag_emb).to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "    # optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
        "    criterion = loss_function_vae\n",
        "\n",
        "    ###############################################################################\n",
        "    # Training code\n",
        "    ###############################################################################\n",
        "\n",
        "    # best_n100 = -np.inf\n",
        "    best_r30 = -np.inf\n",
        "    update_count = 0\n",
        "    early_stopping = 40\n",
        "    stopping_cnt = 0\n",
        "\n",
        "    log_dir_name = str(datetime.datetime.now())[0:10] + '_vae'\n",
        "    log_dir = os.path.join(args.log_dir, log_dir_name)\n",
        "    # log_dir = increment_path(log_dir)\n",
        "\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    # -- wandb initialize with configuration\n",
        "    # wandb.init(config={\"model\":'Multi-VAE',\n",
        "    #                 \"batch_size\": args.batch_size,\n",
        "    #                 \"lr\"        : args.lr,\n",
        "    #                 \"epochs\"    : args.n_epochs,\n",
        "    #                 })\n",
        "\n",
        "    for epoch in range(1, args.n_epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(model, criterion, optimizer, is_VAE=True)\n",
        "        \n",
        "        \n",
        "        val_loss, n100, r10, r30 = evaluate(model, criterion, vad_data_tr, vad_data_te, is_VAE=True)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d}/{:3d} | time: {:4.2f}s | valid loss {:4.4f} | '\n",
        "                'n100 {:5.4f} | r10 {:5.4f} | r30 {:5.4f}'.format(\n",
        "                    epoch, args.n_epochs, time.time() - epoch_start_time, val_loss,\n",
        "                    n100, r10, r30))\n",
        "        print('-' * 89)\n",
        "\n",
        "        n_iter = epoch * len(range(0, N, args.batch_size))\n",
        "\n",
        "        # wandb.log({\n",
        "        #     \"vae_val loss\": val_loss,\n",
        "        #     \"vae_n100\" : n100,\n",
        "        #     \"vae_r10\" : r10,\n",
        "        #     \"vae_r20\" : r20})\n",
        "\n",
        "        if r30 > best_r30:\n",
        "            with open(os.path.join(log_dir, 'best_vae_' + args.save), 'wb') as f:\n",
        "                torch.save(model.state_dict(), f)\n",
        "                print(f\"Best model saved! r@30 : {r30:.4f}\")\n",
        "            best_r30 = r30\n",
        "            stopping_cnt = 0\n",
        "        else:\n",
        "            print(f'Stopping Count : {stopping_cnt} / {early_stopping}')\n",
        "            stopping_cnt += 1\n",
        "\n",
        "        if stopping_cnt > early_stopping:\n",
        "            print('*****Early Stopping*****')\n",
        "            break\n",
        "\n",
        "    # Load the best saved model.\n",
        "    with open(os.path.join(log_dir, 'best_vae_' + args.save), 'rb') as f:\n",
        "        model.load_state_dict(torch.load(f))\n",
        "\n",
        "    with open(os.path.join(args.save_dir, 'best_vae_' + args.save), 'wb') as f:\n",
        "        torch.save(model.state_dict(), f)\n",
        "\n",
        "    # Run on test data.\n",
        "    test_loss, n100, r10, r30 = evaluate(model, criterion, test_data_tr, test_data_te, is_VAE=True)\n",
        "    print('=' * 89)\n",
        "    print('| End of training | test loss {:4.4f} | n100 {:4.4f} | r10 {:4.4f} | '\n",
        "            'r30 {:4.4f}'.format(test_loss, n100, r10, r30))\n",
        "    print('=' * 89)\n",
        "\n",
        "    with open(os.path.join(log_dir, \"update_count_vae.txt\"), \"w\", encoding='utf-8') as f:\n",
        "        f.write(str(update_count))\n",
        "\n",
        "    pro_dir = os.path.join(args.dataset, 'pro_sg')\n",
        "#    np.savetxt(pro_dir+'/vae_result.txt', [r10]) # best_recall 저장\n",
        "\n",
        " #   df_result = pd.read_csv(pro_dir+'/model_score.csv')\n",
        " #   x = pd.DataFrame()\n",
        " #   x['model_name'] = 'vae'\n",
        " #   x['r10'] = r10\n",
        " #   df_result = pd.concat([df_result, x], axis=0)\n",
        " #   df_result.to_csv(pro_dir+'/model_score.csv', index=False)\n",
        "\n",
        "    with open(pro_dir +'/model_score.json', 'r', encoding=\"utf-8\") as f:\n",
        "        model_score = json.load(f)\n",
        "\n",
        "    model_score['vae'] = r10\n",
        "    with open(pro_dir + '/model_score.json', 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(model_score, f, ensure_ascii=False, indent=\"\\t\")"
      ],
      "metadata": {
        "id": "ihCBzbJz2O6Z"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dae.py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "import random\n",
        "from copy import deepcopy\n",
        "#from .utils import *\n",
        "#from .models import *\n",
        "import time\n",
        "#from .config import *\n",
        "#from .dataset import *\n",
        "import datetime\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "seed = args.seed\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "update_count = 0\n",
        "def train_dae():\n",
        "    # 만약 GPU가 사용가능한 환경이라면 GPU를 사용\n",
        "    if torch.cuda.is_available():\n",
        "        args.cuda = True\n",
        "\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    #device = \"cpu\"\n",
        "    print(\"DEVICE: \", device)\n",
        "\n",
        "    def train(model, criterion, optimizer, is_VAE=False):\n",
        "        # Turn on training mode\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        global update_count\n",
        "\n",
        "        np.random.shuffle(idxlist)\n",
        "\n",
        "        for batch_idx, start_idx in enumerate(range(0, N, args.batch_size)):\n",
        "            end_idx = min(start_idx + args.batch_size, N)\n",
        "\n",
        "            data = train_data[idxlist[start_idx:end_idx]]\n",
        "            data = naive_sparse2tensor(data).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if is_VAE:\n",
        "                if args.total_anneal_steps > 0:\n",
        "                    anneal = min(args.anneal_cap,\n",
        "                                1. * update_count / args.total_anneal_steps)\n",
        "                else:\n",
        "                    anneal = args.anneal_cap\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                recon_batch, mu, logvar = model(data)\n",
        "                loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
        "            else:\n",
        "                recon_batch = model(data)\n",
        "                loss = criterion(recon_batch, data)\n",
        "\n",
        "            loss.backward()\n",
        "            train_loss += loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "            update_count += 1\n",
        "\n",
        "    def evaluate(model, criterion, data_tr, data_te, is_VAE=False):\n",
        "        # Turn on evaluation mode\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        global update_count\n",
        "        e_idxlist = list(range(data_tr.shape[0]))\n",
        "        e_N = data_tr.shape[0]\n",
        "        n100_list = []\n",
        "        r10_list = []\n",
        "        r30_list = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for start_idx in range(0, e_N, args.batch_size):\n",
        "                end_idx = min(start_idx + args.batch_size, N)\n",
        "                data = data_tr[e_idxlist[start_idx:end_idx]]\n",
        "                heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
        "\n",
        "                data_tensor = naive_sparse2tensor(data).to(device)\n",
        "                if is_VAE:\n",
        "\n",
        "                    if args.total_anneal_steps > 0:\n",
        "                        anneal = min(args.anneal_cap,\n",
        "                                    1. * update_count / args.total_anneal_steps)\n",
        "                    else:\n",
        "                        anneal = args.anneal_cap\n",
        "\n",
        "                    recon_batch, mu, logvar = model(data_tensor)\n",
        "\n",
        "                    loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
        "\n",
        "                else:\n",
        "                    recon_batch = model(data_tensor)\n",
        "                    loss = criterion(recon_batch, data_tensor)\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Exclude examples from training set\n",
        "                recon_batch = recon_batch.cpu().numpy()\n",
        "                recon_batch[data.nonzero()] = -np.inf\n",
        "\n",
        "                n100 = ndcg(recon_batch, heldout_data, 100)\n",
        "                r10 = recall(recon_batch, heldout_data, 10)\n",
        "                r30 = recall(recon_batch, heldout_data, 30)\n",
        "\n",
        "                n100_list.append(n100)\n",
        "                r10_list.append(r10)\n",
        "                r30_list.append(r30)\n",
        "\n",
        "        total_loss /= len(range(0, e_N, args.batch_size))\n",
        "        n100_list = np.concatenate(n100_list)\n",
        "        r10_list = np.concatenate(r10_list)\n",
        "        r30_list = np.concatenate(r30_list)\n",
        "\n",
        "        return total_loss, np.mean(n100_list), np.mean(r10_list), np.mean(r30_list)\n",
        "\n",
        "\n",
        "    ###############################################################################\n",
        "    # Load data\n",
        "    ###############################################################################\n",
        "    loader = DataLoader(args.dataset)\n",
        "\n",
        "    n_items = loader.load_n_items()  # 6807\n",
        "    train_data = loader.load_data('train')  # csr_matrix\n",
        "    vad_data_tr, vad_data_te = loader.load_data('validation')\n",
        "    test_data_tr, test_data_te = loader.load_data('test')\n",
        "\n",
        "    N = train_data.shape[0]  # 25360\n",
        "    idxlist = list(range(N))\n",
        "\n",
        "    ###############################################################################\n",
        "    # Build the model\n",
        "    ###############################################################################\n",
        "\n",
        "    p_dims = [200, 3000, n_items] \n",
        "    pro_dir2 = os.path.join(args.dataset, 'pro_sg')\n",
        "    item_tag_emb = pd.read_csv(pro_dir2 + '/item_tag_emb.csv')\n",
        "    model = MultiDAE(p_dims, tag_emb=item_tag_emb).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "    # optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)\n",
        "    criterion = loss_function_dae\n",
        "\n",
        "    ###############################################################################\n",
        "    # Training code\n",
        "    ###############################################################################\n",
        "\n",
        "    # best_n100 = -np.inf\n",
        "    best_r30 = -np.inf\n",
        "    update_count = 0\n",
        "    early_stopping = 40\n",
        "    stopping_cnt = 0\n",
        "\n",
        "    log_dir_name = str(datetime.datetime.now())[0:10] + '_dae'\n",
        "    log_dir = os.path.join(args.log_dir, log_dir_name)\n",
        "\n",
        "\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "\n",
        "    for epoch in range(1, args.n_epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(model, criterion, optimizer, is_VAE=False)\n",
        "\n",
        "        val_loss, n100, r10, r30 = evaluate(model, criterion, vad_data_tr, vad_data_te, is_VAE=False)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d}/{:3d} | time: {:4.2f}s | valid loss {:4.4f} | '\n",
        "            'n100 {:5.4f} | r10 {:5.4f} | r30 {:5.4f}'.format(\n",
        "            epoch, args.n_epochs, time.time() - epoch_start_time, val_loss,\n",
        "            n100, r10, r30))\n",
        "        print('-' * 89)\n",
        "\n",
        "        n_iter = epoch * len(range(0, N, args.batch_size))\n",
        "\n",
        "        if r30 > best_r30:\n",
        "            with open(os.path.join(log_dir, 'best_dae_' + args.save), 'wb') as f:\n",
        "                torch.save(model.state_dict(), f)\n",
        "                print(f\"Best model saved! r@30 : {r30:.4f}\")\n",
        "            best_r30 = r30\n",
        "            stopping_cnt = 0\n",
        "        else:\n",
        "            print(f'Stopping Count : {stopping_cnt} / {early_stopping}')\n",
        "            stopping_cnt += 1\n",
        "\n",
        "        if stopping_cnt > early_stopping:\n",
        "            print('*****Early Stopping*****')\n",
        "            break\n",
        "\n",
        "    # Load the best saved model.\n",
        "    with open(os.path.join(log_dir, 'best_dae_' + args.save), 'rb') as f:\n",
        "        model.load_state_dict(torch.load(f))\n",
        "\n",
        "    with open(os.path.join(args.save_dir, 'best_dae_' + args.save), 'wb') as f:\n",
        "        torch.save(model.state_dict(), f)\n",
        "\n",
        "    # Run on test data.\n",
        "    test_loss, n100, r10, r30 = evaluate(model, criterion, test_data_tr, test_data_te, is_VAE=False)\n",
        "    print('=' * 89)\n",
        "    print('| End of training | test loss {:4.4f} | n100 {:4.4f} | r10 {:4.4f} | '\n",
        "        'r30 {:4.4f}'.format(test_loss, n100, r10, r30))\n",
        "    print('=' * 89)\n",
        "\n",
        "    with open(os.path.join(log_dir, \"update_count_dae.txt\"), \"w\", encoding='utf-8') as f:\n",
        "        f.write(str(update_count))\n",
        "\n",
        "    \n",
        "    pro_dir = os.path.join(args.dataset, 'pro_sg')\n",
        "#    np.savetxt(pro_dir+'/dae_result.txt', [r10]) # best_recall 저장\n",
        "\n",
        "\n",
        "#    df_result = pd.read_csv(pro_dir+'/model_score.csv')\n",
        "#    x = pd.DataFrame()\n",
        "#    x['model_name'] = 'dae'\n",
        "#    x['r10'] = r10\n",
        "#    df_result = pd.concat([df_result, x], axis=0)\n",
        "#    df_result.to_csv(pro_dir+'/model_score.csv', index=False)\n",
        "\n",
        "    with open(pro_dir + '/model_score.json', 'r', encoding=\"utf-8\") as f:\n",
        "        model_score = json.load(f)\n",
        "\n",
        "    model_score['dae'] = r10\n",
        "    with open(pro_dir + '/model_score.json', 'w', encoding=\"utf-8\") as f:\n",
        "        json.dump(model_score, f, ensure_ascii=False, indent=\"\\t\")"
      ],
      "metadata": {
        "id": "4xlvqXyG2btz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main.py"
      ],
      "metadata": {
        "id": "3vGI000E2rqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from .preprocessing import preprocessing_all\n",
        "#from .inference import infer_all\n",
        "#from .train_recvae import train_recvae\n",
        "#from .train_vae import train_vae\n",
        "#from .train_dae import train_dae\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from .utils import *\n",
        "#from .config import *\n",
        "import json"
      ],
      "metadata": {
        "id": "RKu7sgFRrA6C"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_users = pd.read_csv('/gdrive/My Drive/Colab Notebooks/SSAFY/users.csv')  # 유저 데이터\n",
        "#df_problems = pd.read_csv('/gdrive/My Drive/Colab Notebooks/SSAFY/problems.csv')  # 문제 데이터\n",
        "#df_problems_solved = pd.read_csv('/gdrive/My Drive/Colab Notebooks/SSAFY/user_solved_problems_fixed.csv')  # 유저별 푼 문제 데이터\n",
        "#df_records_solved = pd.read_csv('/gdrive/My Drive/Colab Notebooks/SSAFY/user_lately_solved_problems2.csv')  # 유저별 최근 60개의 '맞혔습니다' 데이터"
      ],
      "metadata": {
        "id": "c5Gi9X2Wq6aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_data(db):\n",
        "    #df_problems_solved = pd.read_sql('SELECT * FROM problems_solved', db)\n",
        "    df_problems_solved = pd.read_csv('/gdrive/My Drive/Colab Notebooks/SSAFY/user_solved_problems_fixed.csv')  # 유저별 푼 문제 데이터\n",
        "    df_problems_solved['problems'] = df_problems_solved['problems'].str.split(',')\n",
        "    df_problems_solved = df_problems_solved.replace(['', 'null'], [np.nan, np.nan])\n",
        "    df_problems_solved = df_problems_solved.explode('problems').dropna().reset_index(drop=True)\n",
        "    df_problems_solved = df_problems_solved.replace(['', 'null'], [np.nan, np.nan])\n",
        "    df_problems_solved = df_problems_solved.dropna()\n",
        "    df_problems_solved = df_problems_solved.drop(columns=['id'])\n",
        "    df_problems_solved = df_problems_solved.astype({'handle':'str', 'problems':'int'})\n",
        "    df_problems_solved.columns = ['user', 'item']\n",
        "\n",
        "    raw_data, user_activity, item_popularity = filter_triplets(df_problems_solved, 5, 10)\n",
        "    #---------------------------\n",
        "    # 여기에 tag == None인 아이템은 제외\n",
        "    #df_problems = pd.read_sql('select * from problems', db)\n",
        "    df_problems = pd.read_csv('/gdrive/My Drive/Colab Notebooks/SSAFY/problems.csv')  # 문제 데이터\n",
        "    df_problems.drop(df_problems[df_problems.average_tries == 7340].index, axis=0, inplace=True)\n",
        "    # level 0에 해당하는 문제 제거\n",
        "    df_problems = df_problems[df_problems.level != 0]\n",
        "    # not_solvable == False만\n",
        "    df_problems = df_problems[df_problems.is_solvable == True]\n",
        "    # tag가 nan인 문제 제거\n",
        "    df_problems = df_problems[~df_problems.tags.isnull()]\n",
        "\n",
        "    gudegi = [24900, 24901, 24902, 24903, 24904, 24905, 24906, 24907, 24908, 24909, 24910, 24911, 21292, 21293, 21294, 21295, 21296, 21297, 21298, 21299, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643]\n",
        "    df_problems = df_problems[~df_problems['problemId'].isin(gudegi)].reset_index(drop=True)\n",
        "\n",
        "    raw_data = raw_data[raw_data['item'].isin(df_problems['problemId'].values)].reset_index(drop=True)\n",
        "    #---------------------------\n",
        "\n",
        "    return raw_data, df_problems\n",
        "\n",
        "\n",
        "def general_problem_preprocessing(db):\n",
        "    raw_data, df_problems = make_train_data(db)\n",
        "\n",
        "    print(\"Preprocessing Start!!\")\n",
        "    preprocessing_all(raw_data, df_problems, db)\n",
        "\n",
        "\n",
        "def recvae_train():\n",
        "    print(\"Train Start!!\")\n",
        "    train_recvae()\n",
        "\n",
        "def vae_train():\n",
        "    print(\"Train Start!!\")\n",
        "    train_vae()\n",
        "\n",
        "def dae_train():\n",
        "    print(\"Train Start!!\")\n",
        "    train_dae()\n",
        "\n",
        "def general_pb_infer(db):\n",
        "    raw_data, df_problems = make_train_data(db)\n",
        "\n",
        "    print(\"Inference Start!!\")\n",
        "    output = infer_all(raw_data, df_problems, db)\n",
        "    output.item = output.item.astype(str)\n",
        "    output = output.groupby('user')['item'].apply(lambda x: \"%s\" % ','.join(x))\n",
        "    output = pd.DataFrame(output)\n",
        "    output = output.reset_index()\n",
        "\n",
        "    return output\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#   df_problems_solved = 데이터 읽기\n",
        "#   raw_data = make_train_data(df_problems_solved)\n",
        "#   print(from_pre_to_infer(raw_data))"
      ],
      "metadata": {
        "id": "7nlOa_ua2ogA"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}